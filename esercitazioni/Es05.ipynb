{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOFtf3kqYix1lmzW4LnVa80"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Esercizio 1\n","Using all the Python modules you need compute the following distances:\n","\n","a. Compute the $L_p$ distance between $(1, 2)$ and $(3, 4)$ for $p = 1, 2, ∞$.\n","\n","b. Given two objects represented by the tuples $(22, 1, 42, 10)$ and $(20, 0, 36, 8)$:\n","\n","1. Compute the Euclidean distance between the two objects.\n","2. Compute the Manhattan distance between the two objects.\n","3. Compute the Minkowski distance between the two objects, using $h = 3$.\n","4. Compute the supremum distance ($L_∞$) between the two objects.\n"],"metadata":{"id":"d-LVeKYPGl0T"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9PIHv7eRGiEh"},"outputs":[],"source":["from scipy.spatial.distance import minkowski\n","from scipy.spatial.distance import chebyshev\n"]},{"cell_type":"markdown","source":["#Esercizio 2\n","Download the Ionosphere data set from the UCI Machine Learning Repository, remove the class feature and compute the $L_p$ distance between all pairs of data points, for $p=1,2,∞$.\n","\n","Compute the contrast measure on the data set for the different norms.\n","\n","Repeat the exercise after sampling the first $r$ dimensions, where $r$ varies from 1 to the full dimensionality of the data."],"metadata":{"id":"-ucnRDrzGk4c"}},{"cell_type":"code","source":["import pandas as pd\n","\n","labels=list(range(1, 35)) + ['class']\n","df = pd.read_csv('http://www.lacascia.it/bd2/_ionosphere/ionosphere.data', names=labels)\n","data = df.iloc[:, :-1].to_numpy(copy=True)\n","print(data.shape)"],"metadata":{"id":"Q9k29hQwHrfQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Esercizio 3\n","Caricate il musk dataset, eliminate le prime colonne non numerichee e riducete la dimensionalità con PCA tenendo il 99% della varianza.\n","\n","1. Effettuate la classificazione con k-NN utilizzando la distanza di Mahalanobis e confrontate l'accuracy con quella ottenuta con la distanza di default (Euclidea).\n","\n","2. Provate a implementare la match-based similarity ed effettuare di nuovo la classificazione k-NN.\n","\n","(Suggerimento: vedete dalla documentazione come si può utilizzare il KNeighborsClassifier di sklearn con metriche diverse dalla Euclidea o implementate voi l'algoritmo k-NN)"],"metadata":{"id":"xrYR4XRiIb-B"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.decomposition import PCA\n","\n","labels=['molecule', 'conformation']+list(range(166)) +['class']\n","df = pd.read_csv('http://www.lacascia.it/bd2/clean2.data', names=labels)\n","\n","data = df.iloc[:, 2:-1].to_numpy(copy=True)\n","target = df.iloc[:, -1].to_numpy(copy=True)\n","print(target.shape, data.shape)\n","\n","pca = PCA(n_components=0.99)\n","data_n = pca.fit(data).transform(data)\n","print(data_n.shape)"],"metadata":{"id":"efM1k9n1IHLI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Esercizio 4\n","Implementate la shared nearest-neighbor similarity (scegliete voi il numero di vicini da considerare)\n","ed effettuate nuovamente la classificazione k-NN.\n","Se l'accuratezza del classificatore migliora cosa potete ipotizzare sulla distribuzione dei vostri dati."],"metadata":{"id":"4Qv-sGvPYw8w"}},{"cell_type":"code","source":[],"metadata":{"id":"olk3LuPWRB8o"},"execution_count":null,"outputs":[]}]}