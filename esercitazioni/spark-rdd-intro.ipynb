{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1v_Eey21uiDRGTUJ9Y5bRf5DIBb4N6AeZ","timestamp":1702295801956},{"file_id":"https://github.com/pstorniolo/master/blob/main/0101_Hadoop_Spark.ipynb","timestamp":1665231462462}],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","\n","!apt-get upgrade -y #-qq > /dev/null\n","\n","# Install JDK 11\n","!apt-get install openjdk-11-jdk-headless #-qq > /dev/null\n","os.environ[\"JAVA_HOME\"]  = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","!echo; java --version"],"metadata":{"id":"DRvZbPsjBGvm"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w48z9Ca2RKhQ"},"source":["# Install Spark 3.4.0\n","!curl -O https://archive.apache.org/dist/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n","!tar xf spark-3.4.0-bin-hadoop3.tgz\n","!ln -s spark-3.4.0-bin-hadoop3 spark\n","!rm -f *.tgz\n","os.environ[\"SPARK_HOME\"] = \"/content/spark\""],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls -l"],"metadata":{"id":"S28TuhTYBFuO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!echo $SPARK_HOME"],"metadata":{"id":"HIAEUbavjAk6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!echo $JAVA_HOME"],"metadata":{"id":"iU157G9sjOhW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls -l spark/"],"metadata":{"id":"CQTQYBTi7nHk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls -l sample_data/"],"metadata":{"id":"psBJW6yNdKbn"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_vUM_qYTR2vU"},"source":["#Install findspark using pip to make pyspark importable as regular library\n","!pip -q install findspark\n","import findspark\n","findspark.init()\n","\n","#importing pyspark\n","import pyspark"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zcR7zpmtR8nE"},"source":["#importing sparksession\n","from pyspark.sql import SparkSession"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M0iIxwXySB3c"},"source":["#creating a sparksession object and providing appName\n","spark=SparkSession.builder.appName(\"local\").getOrCreate()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dta5v8CSTZxv"},"source":["#printing the version of spark\n","print(\"Apache Spark version: \", spark.version)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FwgAP3VKT0tz"},"source":["!ls -l spark/bin"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6pTsZKZhU3GU"},"source":["from random import randint\n","\n","# create a list of random numbers between 10 to 1000\n","my_large_list = [randint(10,1000) for x in range(0,2000000)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X1-75OQgVPlI"},"source":["sc = spark.sparkContext\n","\n","# create one partition of the list\n","my_large_list_one_partition = sc.parallelize(my_large_list, numSlices=1)\n","\n","# check number of partitions\n","print(my_large_list_one_partition.getNumPartitions())\n","\n","# filter numbers greater than equal to 200\n","my_large_list_one_partition = my_large_list_one_partition.filter(lambda x : x >= 200)\n","\n","# count the number of elements in filtered list\n","print(my_large_list_one_partition.count())"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_large_list_one_partition.cache()"],"metadata":{"id":"f6hGVvpyCCgZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_large_list_one_partition.mean()"],"metadata":{"id":"XFWrIVl0C7YZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UfhyUxLNXiAc"},"source":["# create five partitions of the list\n","my_large_list_with_five_partition = sc.parallelize(my_large_list, numSlices=5)\n","\n","# check number of partitions\n","print(my_large_list_with_five_partition.getNumPartitions())\n","\n","# filter numbers greater than equal to 200\n","my_large_list_with_five_partition = my_large_list_with_five_partition.filter(lambda x : x >= 200)\n","\n","# count the number of elements in the filtered list\n","print(my_large_list_with_five_partition.count())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OT9DbB28Ydyj"},"source":["# create a sample list\n","my_list = [i for i in range(1,10000000)]\n","\n","# parallelize the data\n","rdd_0 = sc.parallelize(my_list,3)\n","\n","# add value 4 to each number\n","rdd_1 = rdd_0.map(lambda x: x + 4)\n","\n","# RDD object\n","rdd_1.take(15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# add value 20 each number\n","rdd_2 = rdd_1.map(lambda x : x + 20)\n","\n","# RDD Object\n","rdd_2.take(15)"],"metadata":{"id":"sD22sUsNDuBf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rdd_2"],"metadata":{"id":"_N-WjW_ZpW3V"},"execution_count":null,"outputs":[]}]}